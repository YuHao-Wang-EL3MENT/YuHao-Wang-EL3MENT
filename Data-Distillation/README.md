<p align="justify"> This is a project focusing on data distillation that can "distill" the origin training dataset into a <b>smaller</b> dataset that can be trained on a network with a much <b>faster</b> training speed. The dataset used in this project is MNIST and FashionMNIST. The origin dataset may contains thousands of image that will consume a large amount of time for a model to trained. With data distillation, the algorithm can extract the key feature in each image and condence them into <b>fewer or even one image for each class</b>.

Here is a output for the synthetic set for MNIST that is generated from noise: 

![vis_DC_MNIST_LeNet_10_iter1000 (1)](https://user-images.githubusercontent.com/110743264/212945840-f8389775-bdd8-41d8-8680-7a083732971a.png)

Below are some results.

| Accuracy      | Scratch | Online-Synthetic-set | Self-Synthetic-set |
|---------------|---------|----------------------|--------------------|
| MNIST         | 0.9937  | 0.9211               | 0.9078             |
| Fashion MNIST | 0.9318  | 0.7553               | 0.5341             |


| Training Time/s | Scratch | Online-Synthetic-set | Self-Synthetic-set |
|---------------|---------|----------------------|--------------------|
| MNIST         | 202.30  | 0.46                 | 0.46               |
| Fashion MNIST | 199.68  | 0.46                 | 0.46               |

<p align="justify">The first table is the accuracy on test set and the second table is the total training time. The "Online-Synthetic-set" is the Off-the-shelf synthetic sets that is generated by the researchers from the https://github.com/VICO-UoE/DatasetCondensation.
For both dataset, the model trained with the synthetic dataset has a drop in the test accuracy but also has a significantly drop in the training time. With a powerful GPU and fine-tunning the generation process, the model trained on the synthetic set will has a better performance.
