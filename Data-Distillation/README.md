<p align="justify"> This is a project focusing on data distillation that can "distill" the origin training dataset into a <b>smaller</b> dataset that can be trained on a network with a much <b>faster</b> training speed. The dataset


<p align="justify"> This is project that aims to distill the knowledge from a teacher model with high structural complexity into a student model with <b>shallower</b> structure and has a <b>similar</b> performance compared with the teacher model. With a <b>simpler structure</b> and a <b>smaller FLOPS</b> (floating point operations per second), the studnet model can be deploy on the device with <b>limited computational power</b> and still has a <b>reasonable performance</b>.
